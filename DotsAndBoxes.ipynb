{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "085zK_ND23p0"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo stable-baselines3 gymnasium supersuit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xvPqEu32f1mr"
      },
      "outputs": [],
      "source": [
        "from gymnasium.spaces import Box, Discrete\n",
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import AgentSelector\n",
        "import numpy as np\n",
        "\n",
        "class DotsAndBoxesEnv(AECEnv):\n",
        "  metadata = {\n",
        "    \"render_modes\": [\"human\"],\n",
        "    \"name\": \"dots_and_boxes\",\n",
        "    \"is_parallelizable\": True\n",
        "  }\n",
        "\n",
        "  def __init__(self, grid_size=5):\n",
        "    super().__init__()\n",
        "    self.grid_size = grid_size\n",
        "    self.total_edges = 2 * (grid_size - 1) * grid_size\n",
        "    self.agents = [\"player_1\", \"player_2\"]\n",
        "    self.possible_agents = self.agents.copy()\n",
        "    self.agent_selector = AgentSelector(self.agents)\n",
        "    self.agent_selection = self.agent_selector.reset()\n",
        "    self.render_mode = None\n",
        "\n",
        "    obs_size = (\n",
        "      self.total_edges +            # Edges\n",
        "      (self.grid_size - 1) ** 2 +   # Box ownerships\n",
        "      (self.grid_size - 1) ** 2 +   # Box completions counts\n",
        "      self.total_edges +            # Action Priorities\n",
        "      2 +                           # Player scores\n",
        "      1                             # Turn\n",
        "    )\n",
        "\n",
        "    self.observation_spaces =  {\n",
        "      \"player_1\": Box(low=0, high=1, shape=(obs_size,), dtype=np.float32),\n",
        "      \"player_2\": Box(low=0, high=1, shape=(obs_size,), dtype=np.float32)\n",
        "    }\n",
        "\n",
        "    self.action_spaces = {\n",
        "      \"player_1\": Discrete(self.total_edges),\n",
        "      \"player_2\": Discrete(self.total_edges)\n",
        "    }\n",
        "\n",
        "  def observe(self, agent):\n",
        "    if agent not in self.agents:\n",
        "      return np.zeros(self.observation_spaces[agent].shape[0], dtype=np.int8)\n",
        "\n",
        "    current = int(agent == self.agent_selection)\n",
        "\n",
        "    box_edges = np.zeros((self.grid_size - 1, self.grid_size - 1), dtype=np.int8)\n",
        "\n",
        "    for i in range(self.grid_size - 1):\n",
        "      for j in range(self.grid_size - 1):\n",
        "        edges = self._edges_for_box(i, j)\n",
        "        box_edges[i, j] = np.sum(self.board_state[edge] for edge in edges)\n",
        "\n",
        "    player_1_score = np.count_nonzero(self.claimed_boxes == 1)\n",
        "    player_2_score = np.count_nonzero(self.claimed_boxes == 2)\n",
        "\n",
        "    action_priorities = self._get_action_priorities()\n",
        "\n",
        "    obs = np.concatenate([\n",
        "      self.board_state.copy(),         # Edges\n",
        "      self.claimed_boxes.flatten(),    # Box ownerships\n",
        "      box_edges.flatten(),             # Box completions counts\n",
        "      action_priorities,               # Action Priorities\n",
        "      [player_1_score, player_2_score],# Player scores\n",
        "      [current]                        # Turn\n",
        "    ]).astype(np.int8)\n",
        "\n",
        "    return obs\n",
        "\n",
        "  def _get_action_priorities(self):\n",
        "    priorities = np.zeros(self.total_edges, dtype=np.int8)\n",
        "\n",
        "    for action in range(self.total_edges):\n",
        "      if self.board_state[action] == 1:\n",
        "        priorities[action] = -2\n",
        "        continue\n",
        "\n",
        "      completing_boxes = 0\n",
        "      three_edges = 0\n",
        "\n",
        "      for i in range(self.grid_size - 1):\n",
        "        for j in range(self.grid_size - 1):\n",
        "          if self.claimed_boxes[i, j] == 0:\n",
        "            edges = self._edges_for_box(i, j)\n",
        "            if action in edges:\n",
        "              current_edges = np.sum(self.board_state[edge] for edge in edges)\n",
        "              if current_edges == 3:\n",
        "                completing_boxes += 1\n",
        "              elif current_edges == 2:\n",
        "                three_edges += 1\n",
        "\n",
        "      if completing_boxes > 0:\n",
        "        priorities[action] = 2\n",
        "      elif three_edges > 0:\n",
        "        priorities[action] = -1\n",
        "      else:\n",
        "        priorities[action] = 1\n",
        "\n",
        "    return priorities\n",
        "\n",
        "  def observation_space(self, agent):\n",
        "    return self.observation_spaces[agent]\n",
        "\n",
        "  def action_space(self, agent):\n",
        "    return self.action_spaces[agent]\n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    self.agents = self.possible_agents.copy()\n",
        "    self.agent_selection = self.agent_selector.reset()\n",
        "    self.board_state = np.zeros(self.total_edges, dtype=np.int8)\n",
        "    self.claimed_boxes = np.zeros((self.grid_size - 1, self.grid_size - 1), dtype=np.int8)\n",
        "\n",
        "    self.rewards = { \"player_1\": 0, \"player_2\": 0 }\n",
        "    self._cumulative_rewards = { \"player_1\": 0, \"player_2\": 0 }\n",
        "    self.terminations = { \"player_1\": False, \"player_2\": False }\n",
        "    self.truncations = { \"player_1\": False, \"player_2\": False }\n",
        "    self.infos = { \"player_1\": {}, \"player_2\": {} }\n",
        "\n",
        "    return {\n",
        "      \"player_1\": self.observe(\"player_1\"),\n",
        "      \"player_2\": self.observe(\"player_2\")\n",
        "    }\n",
        "\n",
        "  def step(self, action):\n",
        "    agent = self.agent_selection\n",
        "\n",
        "    if action is None:\n",
        "      self._was_dead_step(action)\n",
        "      self.agent_selection = self.agent_selector.next()\n",
        "      return\n",
        "\n",
        "    if self.board_state[action] == 1:\n",
        "      self.rewards[agent] = -2\n",
        "      self._accumulate_rewards()\n",
        "      self.agent_selection = self.agent_selector.next()\n",
        "      return\n",
        "\n",
        "    boxes_before = self._get_box_edge_count()\n",
        "    opponent = \"player_1\" if agent == \"player_2\" else \"player_2\"\n",
        "\n",
        "    available_completions = self._count_available_completions(agent)\n",
        "\n",
        "    self.board_state[action] = 1\n",
        "\n",
        "    boxes_after = self._get_box_edge_count()\n",
        "    reward = self._calculate_reward(agent, boxes_before, boxes_after, action)\n",
        "\n",
        "    completed, temp_box_delta = self._check_completed_boxes(agent)\n",
        "\n",
        "    if completed > 0:\n",
        "      reward += completed * 5\n",
        "      self.claimed_boxes += temp_box_delta\n",
        "    else:\n",
        "      if available_completions > 0:\n",
        "        created_three_edges = False\n",
        "        for i in range(self.grid_size - 1):\n",
        "          for j in range(self.grid_size - 1):\n",
        "            if (boxes_before[i, j] == 2 and boxes_after[i, j] == 3 and self.claimed_boxes[i, j] == 0):\n",
        "              created_three_edges = True\n",
        "              break\n",
        "        if created_three_edges:\n",
        "          reward -= 5.0\n",
        "        else:\n",
        "          reward -= 2.0\n",
        "\n",
        "      self.agent_selection = self.agent_selector.next()\n",
        "\n",
        "    self.rewards[agent] = reward\n",
        "\n",
        "    if self.board_state.sum() == self.total_edges:\n",
        "      self.terminations = { \"player_1\": True, \"player_2\": True }\n",
        "\n",
        "      player_1_boxes = np.count_nonzero(self.claimed_boxes == 1)\n",
        "      player_2_boxes = np.count_nonzero(self.claimed_boxes == 2)\n",
        "\n",
        "      if player_1_boxes > player_2_boxes:\n",
        "        self.rewards[\"player_1\"] += 10.0\n",
        "        self.rewards[\"player_2\"] -= 5.0\n",
        "      elif player_2_boxes > player_1_boxes:\n",
        "        self.rewards[\"player_2\"] += 10.0\n",
        "        self.rewards[\"player_1\"] -= 5.0\n",
        "\n",
        "    self._accumulate_rewards()\n",
        "\n",
        "  def _check_completed_boxes(self, agent):\n",
        "    agent_id = 1 if agent == \"player_1\" else 2\n",
        "    completed = 0\n",
        "    temp_box_delta = np.zeros_like(self.claimed_boxes)\n",
        "\n",
        "    for i in range(self.grid_size - 1):\n",
        "      for j in range(self.grid_size - 1):\n",
        "        if self.claimed_boxes[i, j] == 0:\n",
        "          edges = self._edges_for_box(i, j)\n",
        "          if all(self.board_state[edge] == 1 for edge in edges):\n",
        "            completed += 1\n",
        "            temp_box_delta[i, j] = agent_id\n",
        "\n",
        "    return completed, temp_box_delta\n",
        "\n",
        "  def _get_box_edge_count(self):\n",
        "    box_counts = np.zeros((self.grid_size - 1, self.grid_size - 1), dtype=np.int8)\n",
        "    for i in range(self.grid_size - 1):\n",
        "      for j in range(self.grid_size - 1):\n",
        "        edges = self._edges_for_box(i, j)\n",
        "        box_counts[i, j] = np.sum(self.board_state[edge] for edge in edges)\n",
        "    return box_counts\n",
        "\n",
        "  def _count_available_completions(self, agent):\n",
        "    count = 0\n",
        "    for i in range(self.grid_size - 1):\n",
        "      for j in range(self.grid_size - 1):\n",
        "        if self.claimed_boxes[i, j] == 0:\n",
        "          edges = self._edges_for_box(i, j)\n",
        "          edge_counts = np.sum(self.board_state[edge] for edge in edges)\n",
        "          if edge_counts == 3:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "  def _calculate_reward(self, agent, boxes_before, boxes_after, action):\n",
        "    reward = 0.05\n",
        "\n",
        "    affected_boxes = []\n",
        "    for i in range(self.grid_size - 1):\n",
        "      for j in range(self.grid_size - 1):\n",
        "        edges = self._edges_for_box(i, j)\n",
        "        if action in edges:\n",
        "          affected_boxes.append((i, j))\n",
        "\n",
        "    three_edges_box_created = 0\n",
        "    completing_box = False\n",
        "\n",
        "    for i, j in affected_boxes:\n",
        "      edges_before = boxes_before[i, j]\n",
        "      edges_after = boxes_after[i, j]\n",
        "\n",
        "      if edges_before == 2 and edges_after == 3 and self.claimed_boxes[i, j] == 0:\n",
        "        three_edges_box_created += 1\n",
        "        reward -= 3.0\n",
        "\n",
        "      if edges_before == 3 and edges_after == 4:\n",
        "        completing_box = True\n",
        "        reward += 0.5\n",
        "      elif edges_before == 1 and edges_after == 2 and self.claimed_boxes[i, j] == 0:\n",
        "        reward -= 0.2\n",
        "      elif edges_before == 0 and edges_after == 1:\n",
        "        reward += 0.1\n",
        "\n",
        "    if three_edges_box_created > 1:\n",
        "      reward -= 2.0\n",
        "\n",
        "    if completing_box:\n",
        "      available_completions = self._count_available_completions(agent)\n",
        "      if available_completions > 0:\n",
        "        reward += 1.0\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def _edges_for_box(self, row, col):\n",
        "    top = row * (self.grid_size - 1) + col\n",
        "    bottom = (row + 1) * (self.grid_size - 1) + col\n",
        "\n",
        "    left = self.grid_size * (self.grid_size - 1) + row * self.grid_size + col\n",
        "    right = left + 1\n",
        "\n",
        "    return [top, bottom, left, right]\n",
        "\n",
        "  def render(self, mode=None):\n",
        "    size = self.grid_size\n",
        "    h_edges = self.board_state[:size * (size - 1)].reshape((size, size - 1))\n",
        "    v_edges = self.board_state[size * (size - 1):].reshape((size - 1, size))\n",
        "\n",
        "    claimed = self.claimed_boxes.copy()\n",
        "    string = \"\"\n",
        "\n",
        "    for i in range(size):\n",
        "      row = \"\"\n",
        "      for j in range(size - 1):\n",
        "        row += \"•\"\n",
        "        row += \"─\" if h_edges[i, j] == 1 else \" \"\n",
        "      row += \"•\\n\"\n",
        "      string += row\n",
        "\n",
        "      if i < size - 1:\n",
        "        row = \"\"\n",
        "        for j in range(size):\n",
        "          row += \"│\" if v_edges[i, j] == 1 else \" \"\n",
        "          row += \" \" if claimed[i, j] == 0 else (\"1\" if claimed[i, j] == 1 else \"2\")\n",
        "        row += \"\\n\"\n",
        "        string += row\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "      print(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWtUwmpZ6TRJ"
      },
      "outputs": [],
      "source": [
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from pettingzoo.utils import turn_based_aec_to_parallel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def create_env(grid_size=5):\n",
        "    env = DotsAndBoxesEnv(grid_size=grid_size)\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(turn_based_aec_to_parallel(env))\n",
        "    env = ss.concat_vec_envs_v1(env, num_vec_envs=8, base_class=\"stable_baselines3\")\n",
        "    return env\n",
        "\n",
        "env = create_env()\n",
        "\n",
        "# Exploration to understand basic game rules\n",
        "model = PPO(\n",
        "  MlpPolicy,\n",
        "    env,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=4096,\n",
        "    batch_size=512,\n",
        "    n_epochs=15,\n",
        "    gamma=0.95,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    ent_coef=0.05,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    policy_kwargs=dict(\n",
        "        net_arch=[512, 512, 256],\n",
        "        activation_fn=torch.nn.ReLU\n",
        "    ),\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=800_000, progress_bar=True)\n",
        "\n",
        "# Reduce exploration and focus on strategy refinement\n",
        "model.learning_rate = 1e-4\n",
        "model.ent_coef = 0.02\n",
        "model.gamma = 0.98\n",
        "\n",
        "model.learn(total_timesteps=1_000_000, progress_bar=True)\n",
        "\n",
        "# Fine tuning with higher stability and lower exploration\n",
        "model.learning_rate = 3e-5\n",
        "model.ent_coef = 0.005\n",
        "model.gamma = 0.99\n",
        "\n",
        "model.learn(total_timesteps=500_000, progress_bar=True)\n",
        "model.save(\"models/dots_and_boxes_model\")\n",
        "\n",
        "print(\"Training completed\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}